name: Setup Kubernetes Cluster and worker node

on:
  workflow_dispatch:

jobs:
  setup_cluster_and_worker:
    runs-on: ubuntu-latest
    environment: development

    steps:
      - uses: actions/checkout@v3
      - name: setup cluster
        uses: appleboy/ssh-action@master
        with:
          host: ${{ vars.AWS_EC2_CONTROL_PLANE_IP }}
          username: ${{ vars.AWS_EC2_USERNAME }}
          key: ${{ secrets.AWS_EC2_PRIVATE_KEY }}
          script: |
            sudo kubeadm init --pod-network-cidr=192.168.0.0/16 --ignore-preflight-errors=NumCPU,Mem
            
            mkdir -p $HOME/.kube
            sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
            sudo chown $(id -u):$(id -g) $HOME/.kube/config
            
            kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/tigera-operator.yaml
            kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/custom-resources.yaml
            
            kubectl taint nodes --all node-role.kubernetes.io/control-plane-
            
            kubectl apply -f https://projectcontour.io/quickstart/contour.yaml

      - name: get join command for worker node
        id: get_join_command
        uses: appleboy/ssh-action@master
        with:
          host: ${{ vars.AWS_EC2_CONTROL_PLANE_IP }}
          username: ${{ vars.AWS_EC2_USERNAME }}
          key: ${{ secrets.AWS_EC2_PRIVATE_KEY }}
          script: |
            output=$(kubeadm token create --print-join-command)
            echo "::set-output name=join::$output"

      - name: join worker node to cluster
        uses: appleboy/ssh-action@master
        with:
          host: ${{ vars.AWS_EC2_WORKER_NODE_1_IP }}
          username: ${{ vars.AWS_EC2_USERNAME }}
          key: ${{ secrets.AWS_EC2_PRIVATE_KEY }}
          script: |
            sudo ${{ steps.get_join_command.outputs.join }}

      - name: setup NFS share
        uses: appleboy/ssh-action@master
        with:
          host: ${{ vars.AWS_EC2_CONTROL_PLANE_IP }}
          username: ${{ vars.AWS_EC2_USERNAME }}
          key: ${{ secrets.AWS_EC2_PRIVATE_KEY }}
          script: |
            sudo mkdir -p /mnt/nfs/promdata
            sudo chown nobody:nogroup /mnt/nfs/promdata
            
            sudo mkdir -p /mnt/nfs/default_data
            sudo chown nobody:nogroup /mnt/nfs/default_data
            
            export CLIENT_IP1=${{ vars.AWS_EC2_CONTROL_PLANE_PRIVATE_IP }}
            export CLIENT_IP2=${{ vars.AWS_EC2_WORKER_NODE_1_PRIVATE_IP }}
            envsubst < system_files/exports > exports
            sudo mv exports /etc/exports
            
            sudo systemctl restart nfs-kernel-server
            
            kubectl create namespace
            helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner --namespace nfs --set nfs.server=172.30.1.55 --set nfs.path=/mnt/nfs/default_data
            
            kubectl apply -f kubernetes/config/default_pvc.yaml
            
